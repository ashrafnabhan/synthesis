# Reviews of DTA studies

Clinical tests are routinely used for diagnosis, confirming or excluding the presence of a disease or condition. They are also used to monitor disease progression, assess prognosis, and screen asymptomatic populations for disease. Any process that yields information used to inform patient management can be regarded as a clinical test. This includes a wide range of processes from history taking and physical examination to complex imaging techniques. The test forms part of the continuum of patient care. New tests are adopted into clinical practice for a number of reasons, including replacement of an existing test (where the new test is expected to reduce the negative impact on the patient, provide better information, or equivalent information for less cost), triage (to decide whether a more expensive or invasive test is necessary), or as an addition to the existing testing protocol.

The ultimate aim of any research on clinical tests should be to determine impact upon patient management and outcome. To date much of the research on clinical tests is in the form of test accuracy studies. The basic aim of test accuracy studies is to assess how well a test can distinguish between people with and without the disease/condition of interest. The outcome measures used describe the probabilistic relationships between positive and negative test results, and the presence or absence of disease, as compared with the best currently available method (i.e. the reference test).

When considering a systematic review of test accuracy studies, it is important to assess whether review findings will be able to provide the information necessary to inform clinical practice. Any review of test accuracy is likely to be of limited value where evidence is lacking that the disease/condition is associated with long-term morbidity or mortality, or where no effective intervention is available.

## The review question

As with all systematic reviews, the development of a clear, well-defined question is essential to maintaining transparency of the review process and to the quality and relevance of the findings. Some aspects of the question require consideration when planning a review of test accuracy.

## Eligibility criteria

**Population**

Clinical tests perform differently in different populations, for example it would generally be inappropriate to evaluate the performance of a test in a secondary care population when the test is mainly used in primary care. Both frequency and severity of the target condition would be expected to be greater in secondary care. It is therefore important to clearly define the population of interest.

The ideal study sample for a test accuracy study is a consecutive or randomly selected series of patients in whom the target condition is suspected, or for screening studies, the target population.

Because participant sampling methods are often poorly reported in test accuracy studies, using the sampling method as an inclusion/exclusion criterion in the review is likely to result in a substantial reduction in available data. It is likely to be more useful to consider the sampling method and/or its reporting as an aspect of study quality and to base the inclusion criteria relating to the population upon participant characteristics. For example in a review comparing the accuracy of different imaging techniques, the inclusion criteria might state that only patients with a specified level of symptoms, representative of those in whom the test would be used for intervention planning, are eligible.

**Index test**

As with any review, the scope of the question can be broad such as 'what is the optimum testing pathway for the diagnosis and follow-up investigation of childhood urinary tract infection (UTI)?' or it can be narrow; for example 'what is the diagnostic accuracy of magnetic resonance angiography (MRA) when compared with intra-arterial x-ray angiography, for the detection of carotid artery stenosis?' The former is likely to include a number of different technologies, addressing multiple target conditions, whereas the latter compares the performance of an alternative (replacement), less invasive or less costly diagnostic technology with that of the reference standard for the detection of a specified target condition. The rate of technological development may be an important consideration; in this latter example inclusion of MRA techniques that are already obsolete in clinical practice, is unlikely to be useful.

Careful consideration should always be given to the equivalence of different analytical techniques when setting inclusion criteria. For example, a systematic review of fecal occult blood tests to screen for colorectal cancer evaluated both immunochemical and colourimetric methods for detecting blood in the feces; though both methods target blood, they cannot be considered equivalent tests.

The traditional concept of test accuracy often implies the dichotomization of data into test results which are classified as positive (target condition present) or negative (target condition absent). Any systematic review of test accuracy will therefore need to consider diagnostic thresholds (points at which results are classified as positive or negative) for each included index test.

**Reference standard**

The reference standard is usually the best test currently available, and is the standard against which the index test is compared. It needs not be the test used routinely in practice (although it can be).

The test accuracy study is based upon a one-sided comparison between the results of the index test and those of the reference standard. Any discrepancy is assumed to arise from error in the index test. Selection of the reference standard is therefore critical to the validity of a test accuracy study and the definition of the diagnostic threshold forms part of that reference standard.

Note that the assumption of 100% accuracy for the reference standard rarely holds true in practice. This represents a fundamental flaw in the test accuracy study design, since the index test can never be deemed to perform better than the reference standard, and its value may therefore be underestimated.

Where several tests are available to diagnose the target condition, there is often no consensus about which test constitutes the reference standard. In such cases a composite reference standard, which combines the results of several available tests to produce a better indicator of true disease status may be used.

In some instances, it is deemed unethical to use an invasive procedure as a reference standard in a study. In such cases, clinical follow-up and final diagnosis may sometimes be used as a reference standard. There will also be occasions when clinical follow-up and final diagnosis provides the most appropriate reference standard. The length of follow-up should ideally be defined in advance. Studies using follow-up and clinical outcome in this way may be viewed as prognostic studies in that they are measuring the accuracy with which the test is able to predict a future event, rather than the accuracy with which it is able to determine current status. Where such studies are included in a systematic review, it is important to define, in advance, what constitutes appropriate follow-up and hence an adequate reference standard.

The index test may also be evaluated against the test currently in practice. Ideally, this should be done by comparing index test and currently available test to the reference standard in the same population.

**Outcome measures**

The primary outcome of interest for any systematic review of test accuracy is the data required to populate 2 x 2 contingency tables. These describe the relationship between the results of the index test and the reference standard at a given diagnostic threshold (point at which results are classified as positive or negative). The table includes the number of true positives (TP: those that have the disease and test positive), false positives (FP: those that do not have the disease and test positive), false negatives (FN: those that do have the disease and test negative) and true negatives (TN: those that do not have the disease and test negative).

|            |          |                    |            |
|------------|----------|--------------------|------------|
|            |          | Reference standard |            |
|            |          | Disease            | No disease |
| Index test | Positive | TP                 | FP         |
|            | Negative | FN                 | TN         |

From the 2 x 2 contingency table (AKA confusion matrix), the following commonly used measures of test performance can be calculated: sensitivity (true positive proportion, A test with a higher sensitivity has a lower type II error rate), specificity (true negative proportion, a test with a higher specificity has a lower type I error rate), diagnostic accuracy (the correctly classified proportion), diagnostic odds ratio, the number needed to diagnose, positive predictive value (precision), negative predictive value, Likelihood ratio of a positive test (LR+), and Likelihood ratio of a negative test (LR-).

In primary studies, a receiver operating characteristic (ROC) curve describes the relationship between 'true positive fraction' (sensitivity) and 'false positive fraction' (1- specificity) for different positivity thresholds. It is used to display the trade-offs between sensitivity and specificity as a result of varying the diagnostic threshold.

In some scenarios (e.g. tests used in population screening) a threshold which skews diagnostic performance may be preferable (e.g. minimizing the number of false negatives at the expense of some increase in the number of false positive results, in conditions/diseases where missing the presence of disease will lead to serious consequences). Overall diagnostic accuracy is summarized by the area under the curve (AUC); the closer the curve is to the upper left hand corner the better the diagnostic performance. The AUC ranges from 0 to 1, with 0.5 indicating a poor test where the accuracy is equivalent to chance.

As with other types of intervention, when assessing the clinical effectiveness of a diagnostic test, it is important to consider all outcome measures which may be relevant to the use of the test in practice. These might include adverse events and the preferences of patients, although inclusion of such information is rare.

**Study design**

There are two basic types of test accuracy study: 'single-gate' which are similar to consecutive series and 'two-gate' which are similar to case-control studies. The term 'two-gate' being used where two sets of inclusion criteria or 'gates' are applied, one for participants who have the target condition and one for those who do not. These designs differ in structure from other cohort and case-control studies in that both are generally cross-sectional in nature.

-   The single-gate design includes participants in whom ***the disease status is unknown***, and compares the results of the index test with those of the reference standard used to confirm diagnosis, i.e. it is broadly representative of the scenario in which the test would be used in practice.

-   The two-gate design compares the results of the index test in patients with an established diagnosis of the target condition with its results in healthy controls or controls with another diagnosis (known status, with respect to the target condition, is therefore treated as the reference standard); i.e. it is unrepresentative of practice and is unlikely to contain the full spectrum of health and disease over which the test would be used.

There are inherent problems with the two-gate design that may lead to bias. The selective inclusion of cases with more advanced disease is likely to lead to over estimations of sensitivity and inclusion of healthy controls is likely to lead to over estimations of specificity. The recruitment of healthy controls from the general population has been associated with two- to three-fold increases in measures of test performance time-to-events derived from a diagnostic cohort design. This over estimation can be increased further when cases of severe disease are used alongside healthy controls. By contrast, where cases are derived from individuals with mild disease, underestimations of sensitivity can result. Where the control group is derived from patients with alternative diagnoses, specificity may be under or overestimated, depending upon the alternative diagnosis. In theory, the two-gate study design could produce a valid estimate of test performance if the cases were sampled to match the reference standard positive patients in a single-gate study (in terms of the spectrum of disease severity) and controls were matched to the reference standard negative patients (in terms of the spectrum of alternative conditions). In practice however, this is difficult to achieve. Whilst two-gate studies are therefore of limited use in assessing how a test is likely to perform in clinical practice, they can be useful in the earlier phases of test development. Where systematic reviews include both single and two-gate study designs, careful consideration should be given to the methods of analysis and the impact of study design should be assessed in any meta-analyses.

## Identifying research evidence

**Sources**

The importance of searching a wide range of databases to avoid missing relevant diagnostic test accuracy studies has been demonstrated, with MEDLINE and EMBASE providing unique records. The reference lists of included studies can also be a useful resource.

**Database searching**

Many electronic databases do not have appropriate indexing terms to label test accuracy studies, and those that do tend not to apply them consistently. They also vary in their design which adds to the difficulty in identification. The problem is compounded by the fact that the original authors might inadequately reporting and labeling their studies as being test accuracy.

The use of filters to identify reports of diagnostic test accuracy studies in electronic databases may miss a considerable number of relevant articles and is therefore not generally considered appropriate. Database searching should concentrate on terms for index tests and target conditions. If further restriction is required, it can be achieved by means of topic specific terms, rather than using a filter. It is hoped, however, that in time, as the issues of reporting and indexing diagnostic, screening, and prognostic studies are more widely realized, the situation will improve allowing the development of more accurate filters.

**Publication bias**

As the data used in studies of test accuracy are often collected as part of routine clinical practice (and in the past have tended not to require formal registration) it has been argued that test accuracy studies are more easily conducted and abandoned than RCTs. They may therefore be particularly susceptible to publication bias.

It has been demonstrated that the unique features of the test accuracy study make the application of the Begg, Egger, and Macaskill tests of funnel plot asymmetry potentially misleading. An alternative approach uses funnel plots of (natural logarithm (ln) DOR) vs. (1/√effective sample size) and tests for asymmetry using related regression or rank correlation tests. It should be noted that the power of all statistical tests for funnel plot asymmetry decreases with increasing heterogeneity of DOR. It should also be noted that factors other than publication bias, for example aspects of study quality and population characteristics, may be associated with sample size.

Given the limitations of current knowledge, to ignore the possibility of publication bias would seem unwise, however, its assessment in reviews of test accuracy is complex.

## Data extraction

The same precautions against reviewer bias and error should be employed whilst extracting data from test accuracy studies as would be applied in any other type of review. Independent checking of 2x2 data is particularly important, as test accuracy studies are often poorly reported, and the production of a 2x2 table from these studies can be challenging.

Some studies may provide the actual results for each test for individual patients. In this case the researcher may need to classify each patient according to the diagnostic thresholds defined in the review protocol.

Studies may provide categorical data, which may represent multiple categories or stages of disease. In this case data will need to be extracted for the numbers of index test positive and negative participants (using the threshold(s) defined in the review protocol, which may include all thresholds reported) with and without the target condition (as defined by the reference standard, using the threshold(s) defined in the review protocol).

There may be instances when the raw data are not reported, but 2x2 data can be calculated from reported accuracy measures and total numbers of diseased or non- diseased patients.

Somewhat more problematic are cases when the data do not 'fit' the 2x2 contingency table model. 'Forcing' data into a 2x2 contingency table, for example by classifying uncertain index test results as FP or FN, may be inappropriate. The contingency table can be extended to form a six cell table, which accommodates uncertain or indeterminate index test results.

The informative value of an indeterminate test result can be assessed using an indeterminate likelihood ratio (or LR+/-), defined as the probability of an indeterminate test result in the presence of disease divided by the probability of an indeterminate test result in the absence of disease.

When index test and reference standard give clear results (i.e., considered determinate), but there is incomplete concordance, the 2x2 table may be expanded to accommodate a more complete clinical picture.

## Risk of bias assessment

Structured appraisal of methodological quality is key to assessing the reliability of test accuracy studies included in a systematic review. Quality assessment should consider the association of individual elements of methodological quality with test accuracy; generating overall 'quality scores' is not recommended.

There are many differences in the design and conduct of diagnostic accuracy studies that can affect the interpretation of their results. Some differences lead to systematic bias such that estimates of diagnostic performance will differ from their true values, others give rise to variation in results between studies, which can limit applicability. The distinction between bias and variation is not always clear, and quality assessment checklists have tended to include items that are pertinent to both. Sources of variation and bias that are potentially relevant when considering studies of test accuracy are described in @tbl-var. Whilst it is clear that variation (e.g. in the demographic characteristics or severity of disease in the study population) can affect the applicability of the results of both individual studies and systematic reviews, there is limited evidence on the effects of design-related biases in primary studies on the results of systematic reviews. Research on the impact of design-related biases is largely a work in progress, being dependent upon the availability of adequate data sets and consistent methods of quality assessment.

|  |  |  |
|:-----------------------|:------------------|:----------------------------|
| Population |  |  |
| Demographic characteristics | Variation | Test may perform differently in different populations. |
| Disease severity | Variation | Differences in disease severity may lead to different estimates of diagnostic performance. |
| Disease prevalence | Variation Bias | The prevalence of the target condition varies with the setting and may affect estimates of diagnostic performance. In settings of higher prevalence, interpreters are more prone to classify test results as abnormal (context bias). |
| Participant selection | Variation | A selection process that may not include a spectrum of patients similar to that in which the test will be used in practice may limit the applicability of study findings. |
| Test methods |  |  |
| Test execution | Variation | Differences in the execution of the index test and/or reference standard can result in different estimates of diagnostic performance; clear reporting of the methods used is therefore important. |
| Technological | Variation development | Diagnostic performance of tests can change over time due to technological improvements. |
| Treatment paradox | Bias | Occurs when treatment is started, based upon the results of one test prior to undertaking the other; thus disease state is potentially altered between tests. |
| Disease progression | Bias | Occurs when there is sufficient time delay between the application of the index test and the reference standard to allow change in the disease state. |
| Application of the reference standard |  |  |
| Use of an inappropriate reference standard | Bias | The error in diagnoses derived from an imperfect reference standard can result in underestimation of the performance of the index test. |
| Differential verification | Bias | Occurs when the diagnosis is verified using different reference standards, depending upon the result of the index test. |
| Partial verification | Bias | Occurs where only a selected sample of participants undergoing the index test also receive the reference standard. |
| Test or diagnostic review | Bias | Where interpretation of either the index test or reference standard may be influenced by knowledge of the results of the other test. Diagnostic review bias may be present when the results of the index test are known to those interpreting the reference standard. Test review bias may be present when the results of the reference standard are known to those interpreting the index test. |
| Clinical review | Bias | The availability of other relevant clinical information (e.g. symptoms, co-morbidities) may also affect estimates of test performance. |
| Incorporation | Bias | Occurs when the result of the index test is used in establishing the final diagnosis (i.e. it forms part of the reference standard). |
| Observer | Variation | The interpretation placed upon a test result may vary between observers and this can affect estimates of test accuracy. The reproducibility of a test within (intra) and between (inter) observers affects its applicability in practice. |
| Analysis |  |  |
| Handling of un-interpretable results | Bias | Diagnostic tests fail or produce un-interpretable results with varying frequency. Study participants for whom a test result could not be obtained are often removed from reported analyses. This may lead to a biased assessment of test performance. |
| Arbitrary choice of threshold value (the diagnostic threshold is derived from the same data set in which test performance is evaluated) | Variation | The choice of a threshold value based upon that which maximizes sensitivity and specificity for the study data may result in exaggerated estimates of test performance. The test may perform less well at the chosen threshold when evaluated in a new independent patient set. |

: Sources of bias and variation in test accuracy studies {#tbl-var}

QUADAS is an evidence-based, validated, quality assessment tool specifically for use in systematic reviews of test accuracy studies. The QUADAS-2 [@Whiting2011] criteria and the sources of bias and variation to which they relate are shown in @tbl-quad. Piloting of the quality assessment process on a small sample of included studies should be done in an attempt to eliminate any discrepancies in understanding between reviewers.

| Domain | Patient Selection | Index Test | Reference Standard | Flow and Timing |
|:--------------|:--------------|:--------------|:--------------|:--------------|
| Description | Describe methods of patient selection. Describe included patients (previous testing. presentation, intended use of index test, and setting) | Describe the index test and how it was conducted and interpreted | Describe the reference standard and how it was conducted and interpreted | Describe any patients who did not receive the index tests or reference standard or who were excluded from the 2 x 2 table (refer to flow diagram).Describe the interval and any interventions between index tests and the reference standard |
| Signaling questions (yes, no, or unclear) | Was a consecutive or random sample of patients enrolled?Was a case-control design avoided?Did the study avoid inappropriate exclusions? | Were the index test results interpreted without knowledge of the results of the reference standard?If a threshold was used, was it prespecified? | Is the reference standard likely to correctly classify the target condition?Were the reference standard results interpreted without knowledge of the results of the index test? | Was there an appropriate interval between index tests and reference standard?Did all patients receive a reference standard?Did all patients receive the same reference standard?Were all patients included in the analysis? |
| Risk of bias (high, low, or unclear) | Could the selection of patients have introduced bias? | Could the conduct or interpretation of the index test have introduced bias? | Could the reference standard, its conduct, or its interpretation have introduced bias? | Could the patient flow have introduced bias? |
| Concerns about applicability (high, low, or unclear) | Are there concerns that the included patients do not match the review question? | Are there concerns that the index test, its conduct, or its interpretation differ from the review question? | Are there concerns that the target condition as defined by the reference standard does not match the review question? |  |

: Risk of Bias and Applicability Judgments in QUADAS-2 {#tbl-quad}

It is worth noting that the information that can be derived from the quality assessment of test accuracy studies is often limited by poor reporting. Where QUADAS-2 items are judged 'unclear' the researcher cannot be certain whether this indicates poor methods with the attendant consequences for bias/variation, or simply poor reporting of a methodologically sound study. The STARD initiative has proposed standards for the reporting of diagnostic accuracy studies. If these standards are widely adopted and lead to a general improvement in the reporting of test accuracy studies, reviewers will increasingly be able to assess methodological quality rather than the quality of reporting.

## Synthesis

A thorough investigation of heterogeneity should be undertaken before deciding if studies are suitable for combining in a meta-analysis and if so what method to use. Clinical and methodological differences such as patient populations, tests, study design and study conduct, should be considered in addition to statistical variation in the accuracy measures reported by studies.

Where a meta-analysis is not considered clinically or statistically meaningful, a structured narrative synthesis can be carried out which can include the presentation of results in one or more graphical formats. For example the results of individual studies can be plotted in ROC space, whether or not a summary curve is included. As well as stratification by index test characteristics, reviews which focus on determining the optimal diagnostic pathway for a condition, rather than the diagnostic performance of a single test, should consider structuring narrative reports to represent the order in which tests would be applied in clinical practice.

**Assessment of statistical heterogeneity**

**Threshold effect**

A source of heterogeneity unique to test accuracy studies, which requires careful assessment, arises from the choice of the threshold used to define a positive result. Even when different thresholds are not explicitly defined, variation in interpretation by observers may result in implicit variation in threshold. This can be assessed visually using a ROC space plot and statistically by measuring the correlation between sensitivity and specificity. However, statistical tests may be unreliable where studies in a systematic review have small sample sizes; threshold effect may be present but undetected by statistical tests. A ROC space plot is a plot of the 'true positive rate' (sensitivity) from each study against the 'false positive rate' (1 - specificity). If a threshold effect exists then the plot will show a curve (as the threshold decreases the sensitivity will increase and the specificity will decrease). This curve follows the operating characteristics of the test at varying thresholds. The presence of a threshold effect can also be investigated using a regression or a hierarchical summary ROC (HSROC) model.

**Heterogeneity of individual diagnostic accuracy measures**

Variability among each of the individual measurements (sensitivity, specificity, positive and negative likelihood ratio, and DOR) can be assessed using the same methods as for other study types. Forest plots can be used to visually assess differences between studies, although these will not show any threshold effects. Paired forest plots should be used when illustrating paired outcome measures such as sensitivity and specificity. Use of statistical tests of heterogeneity does not reliably indicate absence of heterogeneity and it is generally advisable to assume the presence of heterogeneity and to fit models which aim to describe and account for it.

**Meta-analysis**

The meta-analysis of diagnostic accuracy studies requires the use of some specific statistical methods which differ from standard methods. Meta-analysis has two main aims: to obtain a pooled measure of diagnostic accuracy and in the case of summary ROC (SROC) models, to explore the heterogeneity among studies. Diagnostic accuracy is usually represented by a pair of related measurements, for example: sensitivity and specificity; and this relationship needs to be incorporated into the analysis methods.

**Pooling individual diagnostic accuracy measures**

A robust approach to combining data and estimating the underlying relationship between sensitivity and specificity is the construction of a SROC curve. Methods that involve pooling sensitivity and specificity from individual studies, or combining positive and negative likelihood ratios fail to account for the paired nature of the parameters, and should generally be avoided. However, where only one parameter (e.g. sensitivity, but not specificity) is presented, simple pooling of proportions is the only option. Assessment of single parameters is usually inappropriate, but is sometimes used when there is a specific clinical reason why only one parameter should be the focus of interest.

Diagnostic odds ratios (DOR) can be pooled using standard fixed or random-effects methods for pooling odds ratios. However, these methods do not help estimate average sensitivity and specificity and may produce erroneous results where there is a relationship between DOR and threshold.

Predictive values should not be pooled in meta-analyses as they are affected by the prevalence of disease in the populations of the studies. Overall predictive values are sometimes calculated using estimates of prevalence from the included studies and pooled estimates of likelihood ratios. However, the potentially misleading nature of such estimates should be considered carefully.

**Simple methods of estimating summary ROC curves**

The Moses-Littenburg regression based method, has been used as a simple method of pooling study results in the presence of a suspected threshold effect. [@Moses1993] It can be used in preliminary exploratory analyses and is helpful in understanding the data. However, it has limitations and should not be used to obtain summary estimates of sensitivity and specificity.

**Optimal methods of modelling SROC curves**

Statistical models, including hierarchical [@Rutter2001] and bivariate [@Reitsma2005] models, have been developed for the estimation of SROC curves in the meta-analysis of test accuracy results.

The HSROC model accounts for both within- and between-study variation in true positive and false positive rates. The model estimates parameters for the threshold, log DOR and the shape of the underlying ROC curve. The HSROC model can be extended to deal with studies that provide results for more than one threshold.

The bivariate model analyses sensitivity and specificity jointly, therefore retaining the paired nature of the original data. It is possible to fit this model using package "mada" [@mada] in R. [@base] The HSROC and bivariate models have been shown to produce equivalent results in the absence of other study-level covariates.

**Exploring heterogeneity**

Sources of methodological and/or clinical heterogeneity can be explored using subgroup analyses. Ideally subgroups should be planned at the protocol stage. However, where this is dependent upon what data are available, and an adaptive process is needed, this should be stated clearly in the protocol. Results from different groups, for example different tests, or study designs, can be visually assessed by using a ROC space plot with different symbols. @fig-dtasub illustrates the divergent accuracy results between different study designs from a systematic review of fecal occult blood tests used in screening for colorectal cancer, which indicates that two-gate studies (white circles) overestimate test performance compared with single-gate studies (black circles).

![](images/subg.png){#fig-dtasub}

HSROC and bivariate models can be used to assess heterogeneity by including covariates. These models allow investigation of the effect of covariates on sensitivity and specificity separately, rather than just the DOR (although this can still be obtained).

Different methods can be used to explore heterogeneity in systematic reviews of diagnostic test accuracy. It should be noted that, as for meta-regression analyses of other study designs, these analyses are exploratory, can only include covariates reported by the studies, and should not be conducted if there are only a small number of studies (a minimum of 10 studies per covariate is needed). Regardless of the approach used, study-level factors to be examined should be defined in the protocol and aspects of methodological quality, (e.g. QUADAS items) should be considered individually, rather than as overall quality scores.

Methods for calculating outcome measures, assessing heterogeneity, producing plots (both with and without summary estimates) and undertaking exploratory analyses are available in R. [@base]

## Presentation of results

When presenting the results of a systematic review of clinical tests it is important to consider how these results will be understood by clinicians and applied in practice.

The presentation of diagnostic measures should be similar for both narrative and meta-analytic approaches, with graphical representation and/or tabulation of individual study results and additional results presented if meta-analysis was performed. Sufficient detail of the tests, participants, study design and conduct should be presented in tables.

The 2 x 2 table results of TP, FP, FN and TN together with sensitivity and specificity, as a minimum should be presented for each study.

The choice of accuracy measures presented depends on the aims and anticipated users of the review. Sensitivity and specificity and likelihood ratios are measures of test performance; likelihood ratios may be more useful in a clinical setting as they can be used to calculate the probability of disease given a particular test result, whereas DORs are difficult to interpret clinically.

Forest plots or ROC space plots provide useful visual summaries and can be easier to interpret than large tables of numbers. The ranges should be presented when summarizing results which have not been subject to meta-analytic pooling. For paired results it may be useful to also present the corresponding measure for the studies at each end of the range, e.g. 'sensitivity ranged from 48% (at a specificity of 80%) to 92% (at a specificity of 70%)'.

If a meta-analysis was undertaken then the presentation of results depends on the methods used. If sensitivity or specificity have been pooled as individual measures then the summary estimate together with the 95% confidence intervals should be presented.

If an SROC model has been used then the relevant SROC curve(s) should be presented. Where the performance of a number of index tests is being compared it may be useful to present multiple SROC curves (or un-pooled data sets) on the same plot. Summary measures of overall diagnostic accuracy, such as AUC or the Q\* point (the point on the curve where sensitivity and specificity are equal) may also be presented. However, the relevance of the Q\* point is debatable, as its use may lead to summary estimates of sensitivity and specificity outside the values in the original studies. Pairs of sensitivity and specificity values can also be read from the SROC curve and presented as a number of summary points in order to provide an overall description of the curve. The estimated SROC curves should also be presented if HSROC or bivariate models have been used. These models enable the calculation of summary estimates of sensitivity and specificity, which should be reported along with their 95% confidence intervals. Although the use of HSROC or bivariate models to generate summary likelihood ratios is not recommended, where likelihood ratios are considered helpful to interpretation, summary likelihood ratios can be calculated from the pooled estimates of sensitivity and specificity generated by these models. For results from a HSROC or bivariate model, as these retain the paired nature of sensitivity and specificity, a region can be plotted around the summary operating point which represents the 95% confidence intervals of both measures. Confidence interval regions can also be plotted for the results of individual studies, but care is required to ensure that these are not mistakenly interpreted as representations of study weighting. Both models can also be used to plot a prediction region; this is the region which has a particular probability of including the true sensitivity and specificity of a future study.

## Report writing

Systematic reviews of diagnostic test accuracy (DTA) studies are fundamental to the decision making process in evidence based medicine. Although such studies are regarded as high level evidence, these reviews are not always reported completely and transparently. Suboptimal reporting of DTA systematic reviews compromises their validity and generalisability, and subsequently their value to key stakeholders. An extension of the PRISMA (preferred reporting items for systematic review and meta-analysis) statement must be folllowed to improve the reporting quality of DTA systematic reviews. [@McInnes2018] The PRISMA-DTA statement has 27 items, of which eight are unmodified from the original PRISMA statement.

**Summary: Diagnostic studies**

-   Researchers planning systematic reviews of test accuracy should give careful consideration to context.

-   Diagnostic tests should be evaluated in patients who are representative of those in whom the test will be used in practice; ideally a consecutive or randomly selected series whose diagnosis is unknown at the time of testing.

-   Careful consideration should be given to what is the appropriate reference standard to establish diagnosis.

-   Difficulties in searching bibliographic databases for test accuracy studies and the lack of suitable methodological search filters mean that more specific searches carry a risk of missing studies. Searches based upon index test and target condition, which are designed to maximize sensitivity, are therefore recommended.

-   Test accuracy studies are often poorly reported, hampering data extraction, quality assessment and synthesis.

-   Though often unable to provide a definitive estimate of test accuracy, systematic reviews can highlight important gaps in the evidence base and aid in the design of future studies.
